# Large Language Models (LLM)

* Shanahan, M., 2024. [Talking about large language models](https://dl.acm.org/doi/10.1145/3624724). Communications of the ACM, 67(2), pp.68-79.
* Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D. and Christiano, P.F., 2020. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33, pp.3008-3021.
* Devlin, J., Chang, M.W., Lee, K. and Toutanova, K., 2019, June. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of Conference of North American Chapter of Association for Computational Linguistics: Human Language Technologies, Volume 1 (pp. 4171-4186).
* Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, ≈Å. and Polosukhin, I., 2017. Attention is all you need. Advances in Neural Information Processing Systems, 30.
