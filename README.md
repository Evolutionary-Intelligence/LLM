# Large Language Models (LLM)

* Bansal, R., Samanta, B., Dalmia, S., Gupta, N., Vashishth, S., Ganapathy, S., Bapna, A., Jain, P. and Talukdar, P., 2024. Llm augmented llms: Expanding capabilities through composition. arXiv preprint arXiv:2401.02412.
* Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y. and Narasimhan, K., 2024. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36.
* Bachmann, G. and Nagarajan, V., 2024. The pitfalls of next-token prediction. arXiv preprint arXiv:2403.06963.
* Momennejad, I., Hasanbeig, H., Vieira Frujeri, F., Sharma, H., Jojic, N., Palangi, H., Ness, R. and Larson, J., 2024. Evaluating cognitive maps and planning in large language models with CogEval. Advances in Neural Information Processing Systems, 36.
* Valmeekam, K., Marquez, M., Sreedharan, S. and Kambhampati, S., 2024. On the planning abilities of large language models-a critical investigation. Advances in Neural Information Processing Systems, 36.
* https://www.quantamagazine.org/how-selective-forgetting-can-help-ai-learn-better-20240228/
* Duan, H., Dziedzic, A., Papernot, N. and Boenisch, F., 2024. Flocks of stochastic parrots: Differentially private prompt learning for large language models. Advances in Neural Information Processing Systems, 36.
* Shanahan, M., 2024. [Talking about large language models](https://dl.acm.org/doi/10.1145/3624724). Communications of the ACM, 67(2), pp.68-79.
* Weidinger, L., McKee, K.R., Everett, R., Huang, S., Zhu, T.O., Chadwick, M.J., Summerfield, C. and Gabriel, I., 2023. Using the Veil of Ignorance to align AI systems with principles of justice. Proceedings of the National Academy of Sciences, 120(18), p.e2213709120.
* Pallagani, V., Muppasani, B., Murugesan, K., Rossi, F., Srivastava, B., Horesh, L., Fabiano, F. and Loreggia, A., 2023. Understanding the capabilities of large language models for automated planning. arXiv preprint arXiv:2305.16151.
* Malladi, S., Gao, T., Nichani, E., Damian, A., Lee, J.D., Chen, D. and Arora, S., 2023. Fine-tuning language models with just forward passes. Advances in Neural Information Processing Systems.
* Shanahan, M., McDonell, K. and Reynolds, L., 2023. Role play with large language models. Nature, 623(7987), pp.493-498.
* Shen, S., Hou, L., Zhou, Y., Du, N., Longpre, S., Wei, J., Chung, H.W., Zoph, B., Fedus, W., Chen, X. and Vu, T., 2023. Mixture-of-experts meets instruction tuning: A winning combination for large language models. arXiv preprint arXiv:2305.14705.
* Mitchell, M. and Krakauer, D.C., 2023. [The debate over understanding in AI’s large language models](https://www.pnas.org/doi/abs/10.1073/pnas.2215907120). Proceedings of the National Academy of Sciences, 120(13), p.e2215907120.
  * "The current debate suggests a fascinating divergence in how to think about understanding in intelligent systems, in particular the contrast between mental models that rely on statistical correlations and those that rely on causal mechanisms."
  * "Indeed, the trajectory of human understanding— both individual and collective—is the development of highly compressed, causally based models of the world analogous to the progression from Ptolemy’s epicycles to Kepler’s elliptical orbits and to Newton’s concise and causal account of planetary motion in terms of gravity."
* Achiam, J., Adler, S., Agarwal, S., and et al., 2023. [GPT-4 technical report](https://arxiv.org/abs/2303.08774?). arXiv preprint arXiv:2303.08774.
* Gulcehre, C., Paine, T.L., Srinivasan, S., Konyushkova, K., Weerts, L., Sharma, A., Siddhant, A., Ahern, A., Wang, M., Gu, C. and Macherey, W., 2023. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998.
* Sejnowski, T.J., 2023. Large language models and the reverse turing test. Neural Computation, 35(3), pp.309-342.
* Mahowald, K., Ivanova, A.A., Blank, I.A., Kanwisher, N., Tenenbaum, J.B. and Fedorenko, E., 2023. Dissociating language and thought in large language models: A cognitive perspective. arXiv preprint arXiv:2301.06627.
* Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K. and Herzog, A., 2022. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691.
* Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A. and Schulman, J., 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35, pp.27730-27744.
* Srivastava, A., Rastogi, A., Rao, A., Shoeb, A.A.M., Abid, A., Fisch, A., Brown, A.R., Santoro, A., Gupta, A., Garriga-Alonso, A. and Kluska, A., 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.
* Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D. and Chi, E.H., 2022. Emergent abilities of large language models. Transactions on Machine Learning Research.
* Arora, K., Asri, L.E., Bahuleyan, H. and Cheung, J.C.K., 2022. Why exposure bias matters: An imitation learning perspective of error accumulation in language generation. arXiv preprint arXiv:2204.01171.
* Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P.S., Cheng, M., Glaese, M., Balle, B., Kasirzadeh, A. and Kenton, Z., 2021. [Ethical and social risks of harm from language models](https://arxiv.org/abs/2112.04359). arXiv preprint arXiv:2112.04359.
  * Chouldechova, A. and Roth, A., 2020. A snapshot of the frontiers of fairness in machine learning. Communications of the ACM, 63(5), pp.82-89.
  * Garg, S., Perot, V., Limtiaco, N., Taly, A., Chi, E.H. and Beutel, A., 2019, January. Counterfactual fairness in text classification through robustness. In Proceedings of AAAI/ACM Conference on AI, Ethics, and Society (pp. 219-226).
  * Zou, J. and Schiebinger, L., 2018. AI can be sexist and racist—it's time to make it fair. Nature, 559(7714), pp.324-326.
  * Caliskan, A., Bryson, J.J. and Narayanan, A., 2017. Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334), pp.183-186.
* Bommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M.S., Bohg, J., Bosselut, A., Brunskill, E. and Brynjolfsson, E., 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.
* Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A. and Agarwal, S., 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33, pp.1877-1901.
* Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D. and Christiano, P.F., 2020. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33, pp.3008-3021.
* Niven, T. and Kao, H.Y., 2019. Probing neural network comprehension of natural language arguments. arXiv preprint arXiv:1907.07355.
* Devlin, J., Chang, M.W., Lee, K. and Toutanova, K., 2019, June. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of Conference of North American Chapter of Association for Computational Linguistics: Human Language Technologies, Volume 1 (pp. 4171-4186).
* Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. and Polosukhin, I., 2017. [Attention is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html). Advances in Neural Information Processing Systems.
* Halevy, A., Norvig, P. and Pereira, F., 2009. The unreasonable effectiveness of data. IEEE Intelligent Systems, 24(2), pp.8-12.
