# Large Language Models (LLM)

* https://www.quantamagazine.org/how-selective-forgetting-can-help-ai-learn-better-20240228/
* Duan, H., Dziedzic, A., Papernot, N. and Boenisch, F., 2024. Flocks of stochastic parrots: Differentially private prompt learning for large language models. Advances in Neural Information Processing Systems, 36.
* Shanahan, M., 2024. [Talking about large language models](https://dl.acm.org/doi/10.1145/3624724). Communications of the ACM, 67(2), pp.68-79.
* Malladi, S., Gao, T., Nichani, E., Damian, A., Lee, J.D., Chen, D. and Arora, S., 2023. Fine-tuning language models with just forward passes. Advances in Neural Information Processing Systems.
* Shanahan, M., McDonell, K. and Reynolds, L., 2023. Role play with large language models. Nature, 623(7987), pp.493-498.
* Mitchell, M. and Krakauer, D.C., 2023. [The debate over understanding in AI’s large language models](https://www.pnas.org/doi/abs/10.1073/pnas.2215907120). Proceedings of the National Academy of Sciences, 120(13), p.e2215907120.
* Achiam, J., Adler, S., Agarwal, S., and et al., 2023. [GPT-4 technical report](https://arxiv.org/abs/2303.08774?). arXiv preprint arXiv:2303.08774.
* Gulcehre, C., Paine, T.L., Srinivasan, S., Konyushkova, K., Weerts, L., Sharma, A., Siddhant, A., Ahern, A., Wang, M., Gu, C. and Macherey, W., 2023. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998.
* Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A. and Schulman, J., 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35, pp.27730-27744.
* Bommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M.S., Bohg, J., Bosselut, A., Brunskill, E. and Brynjolfsson, E., 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.
* Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A. and Agarwal, S., 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33, pp.1877-1901.
* Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D. and Christiano, P.F., 2020. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33, pp.3008-3021.
* Devlin, J., Chang, M.W., Lee, K. and Toutanova, K., 2019, June. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of Conference of North American Chapter of Association for Computational Linguistics: Human Language Technologies, Volume 1 (pp. 4171-4186).
* Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. and Polosukhin, I., 2017. Attention is all you need. Advances in Neural Information Processing Systems.
* Halevy, A., Norvig, P. and Pereira, F., 2009. The unreasonable effectiveness of data. IEEE Intelligent Systems, 24(2), pp.8-12.
